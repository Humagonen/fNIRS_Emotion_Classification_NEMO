{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b770103f-a773-4e67-94ab-b35eadd67425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b56147e-1a61-41b4-a09f-60b41877bda5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>label</th>\n",
       "      <th>S1_D1_w1</th>\n",
       "      <th>S1_D1_w2</th>\n",
       "      <th>S1_D1_w3</th>\n",
       "      <th>S1_D2_w1</th>\n",
       "      <th>S1_D2_w2</th>\n",
       "      <th>S1_D2_w3</th>\n",
       "      <th>S2_D1_w1</th>\n",
       "      <th>S2_D1_w2</th>\n",
       "      <th>...</th>\n",
       "      <th>S9_D6_w3</th>\n",
       "      <th>S9_D8_w1</th>\n",
       "      <th>S9_D8_w2</th>\n",
       "      <th>S9_D8_w3</th>\n",
       "      <th>S10_D7_w1</th>\n",
       "      <th>S10_D7_w2</th>\n",
       "      <th>S10_D7_w3</th>\n",
       "      <th>S10_D8_w1</th>\n",
       "      <th>S10_D8_w2</th>\n",
       "      <th>S10_D8_w3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sub-101</td>\n",
       "      <td>2</td>\n",
       "      <td>-7.120779e-08</td>\n",
       "      <td>-1.591482e-08</td>\n",
       "      <td>4.037125e-08</td>\n",
       "      <td>1.059506e-07</td>\n",
       "      <td>-1.591482e-08</td>\n",
       "      <td>5.576864e-09</td>\n",
       "      <td>-5.745610e-08</td>\n",
       "      <td>1.158969e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.338075e-08</td>\n",
       "      <td>4.680183e-08</td>\n",
       "      <td>-6.375286e-08</td>\n",
       "      <td>1.427296e-08</td>\n",
       "      <td>4.680183e-08</td>\n",
       "      <td>-7.726111e-09</td>\n",
       "      <td>1.427296e-08</td>\n",
       "      <td>-7.726111e-09</td>\n",
       "      <td>1.427296e-08</td>\n",
       "      <td>-7.726111e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sub-101</td>\n",
       "      <td>0</td>\n",
       "      <td>-7.105098e-08</td>\n",
       "      <td>-1.428847e-07</td>\n",
       "      <td>-1.661490e-07</td>\n",
       "      <td>-1.067971e-07</td>\n",
       "      <td>-1.428847e-07</td>\n",
       "      <td>-1.549473e-07</td>\n",
       "      <td>-1.100738e-07</td>\n",
       "      <td>-9.783376e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.825674e-08</td>\n",
       "      <td>-1.156452e-07</td>\n",
       "      <td>-1.293089e-07</td>\n",
       "      <td>-1.468958e-07</td>\n",
       "      <td>-1.156452e-07</td>\n",
       "      <td>-1.101812e-07</td>\n",
       "      <td>-1.468958e-07</td>\n",
       "      <td>-1.101812e-07</td>\n",
       "      <td>-1.468958e-07</td>\n",
       "      <td>-1.101812e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sub-101</td>\n",
       "      <td>3</td>\n",
       "      <td>6.854295e-08</td>\n",
       "      <td>1.563502e-07</td>\n",
       "      <td>1.207582e-07</td>\n",
       "      <td>1.066198e-07</td>\n",
       "      <td>1.563502e-07</td>\n",
       "      <td>1.385606e-07</td>\n",
       "      <td>1.212980e-07</td>\n",
       "      <td>4.591666e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>4.139978e-08</td>\n",
       "      <td>2.738268e-08</td>\n",
       "      <td>7.855582e-08</td>\n",
       "      <td>1.718898e-08</td>\n",
       "      <td>2.738268e-08</td>\n",
       "      <td>4.968935e-09</td>\n",
       "      <td>1.718898e-08</td>\n",
       "      <td>4.968935e-09</td>\n",
       "      <td>1.718898e-08</td>\n",
       "      <td>4.968935e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sub-101</td>\n",
       "      <td>1</td>\n",
       "      <td>1.206235e-07</td>\n",
       "      <td>1.431428e-07</td>\n",
       "      <td>2.246828e-08</td>\n",
       "      <td>2.433219e-08</td>\n",
       "      <td>1.431428e-07</td>\n",
       "      <td>1.236011e-07</td>\n",
       "      <td>1.035702e-07</td>\n",
       "      <td>1.868319e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.115965e-07</td>\n",
       "      <td>-2.759947e-08</td>\n",
       "      <td>-1.808735e-07</td>\n",
       "      <td>5.235214e-08</td>\n",
       "      <td>-2.759947e-08</td>\n",
       "      <td>1.151776e-08</td>\n",
       "      <td>5.235214e-08</td>\n",
       "      <td>1.151776e-08</td>\n",
       "      <td>5.235214e-08</td>\n",
       "      <td>1.151776e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sub-101</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.319688e-07</td>\n",
       "      <td>-1.376738e-07</td>\n",
       "      <td>-2.001670e-07</td>\n",
       "      <td>-1.707070e-07</td>\n",
       "      <td>-1.376738e-07</td>\n",
       "      <td>-6.784068e-08</td>\n",
       "      <td>-1.378276e-07</td>\n",
       "      <td>-6.926143e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.588279e-07</td>\n",
       "      <td>-1.109203e-07</td>\n",
       "      <td>-2.869832e-07</td>\n",
       "      <td>-1.373283e-07</td>\n",
       "      <td>-1.109203e-07</td>\n",
       "      <td>-1.356308e-07</td>\n",
       "      <td>-1.373283e-07</td>\n",
       "      <td>-1.356308e-07</td>\n",
       "      <td>-1.373283e-07</td>\n",
       "      <td>-1.356308e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject  label      S1_D1_w1      S1_D1_w2      S1_D1_w3      S1_D2_w1  \\\n",
       "0  sub-101      2 -7.120779e-08 -1.591482e-08  4.037125e-08  1.059506e-07   \n",
       "1  sub-101      0 -7.105098e-08 -1.428847e-07 -1.661490e-07 -1.067971e-07   \n",
       "2  sub-101      3  6.854295e-08  1.563502e-07  1.207582e-07  1.066198e-07   \n",
       "3  sub-101      1  1.206235e-07  1.431428e-07  2.246828e-08  2.433219e-08   \n",
       "4  sub-101      2 -1.319688e-07 -1.376738e-07 -2.001670e-07 -1.707070e-07   \n",
       "\n",
       "       S1_D2_w2      S1_D2_w3      S2_D1_w1      S2_D1_w2  ...      S9_D6_w3  \\\n",
       "0 -1.591482e-08  5.576864e-09 -5.745610e-08  1.158969e-09  ... -6.338075e-08   \n",
       "1 -1.428847e-07 -1.549473e-07 -1.100738e-07 -9.783376e-08  ... -9.825674e-08   \n",
       "2  1.563502e-07  1.385606e-07  1.212980e-07  4.591666e-08  ...  4.139978e-08   \n",
       "3  1.431428e-07  1.236011e-07  1.035702e-07  1.868319e-09  ... -1.115965e-07   \n",
       "4 -1.376738e-07 -6.784068e-08 -1.378276e-07 -6.926143e-08  ... -1.588279e-07   \n",
       "\n",
       "       S9_D8_w1      S9_D8_w2      S9_D8_w3     S10_D7_w1     S10_D7_w2  \\\n",
       "0  4.680183e-08 -6.375286e-08  1.427296e-08  4.680183e-08 -7.726111e-09   \n",
       "1 -1.156452e-07 -1.293089e-07 -1.468958e-07 -1.156452e-07 -1.101812e-07   \n",
       "2  2.738268e-08  7.855582e-08  1.718898e-08  2.738268e-08  4.968935e-09   \n",
       "3 -2.759947e-08 -1.808735e-07  5.235214e-08 -2.759947e-08  1.151776e-08   \n",
       "4 -1.109203e-07 -2.869832e-07 -1.373283e-07 -1.109203e-07 -1.356308e-07   \n",
       "\n",
       "      S10_D7_w3     S10_D8_w1     S10_D8_w2     S10_D8_w3  \n",
       "0  1.427296e-08 -7.726111e-09  1.427296e-08 -7.726111e-09  \n",
       "1 -1.468958e-07 -1.101812e-07 -1.468958e-07 -1.101812e-07  \n",
       "2  1.718898e-08  4.968935e-09  1.718898e-08  4.968935e-09  \n",
       "3  5.235214e-08  1.151776e-08  5.235214e-08  1.151776e-08  \n",
       "4 -1.373283e-07 -1.356308e-07 -1.373283e-07 -1.356308e-07  \n",
       "\n",
       "[5 rows x 74 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv(\"all_subs_windowed.csv\")\n",
    "\n",
    "df['label'] = df['label'] - 1   # convert labels (1-4 to 0-3)\n",
    "df.head()  # 1203 rows full data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1624191c-34f3-416a-bae9-8e73bfa51024",
   "metadata": {},
   "source": [
    "## LOSO CV - base models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9eb05d8-fa10-4001-b3e3-bf8dc2e75354",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31955b9f-8758-425a-9cb2-4216061b7b84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\humag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:46:17] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\humag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:46:21] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\humag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:46:25] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\humag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:46:31] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\humag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:46:35] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\humag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:46:38] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\humag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:46:41] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\humag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:46:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\humag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:46:49] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\humag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:46:53] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\humag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:46:56] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\humag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:47:04] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\humag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:47:07] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\humag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:47:10] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\humag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:47:16] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\humag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:47:21] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\humag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:47:25] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\humag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:47:31] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\humag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:47:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\humag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:47:40] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\humag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:47:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\humag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:47:49] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\humag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:47:52] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\humag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:47:56] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\humag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:48:02] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\humag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:48:05] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\humag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:48:08] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\humag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:48:12] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\humag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:48:18] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\humag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:48:21] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Mean Accuracy (LOSO): 0.2635\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Features, label, and subject group\n",
    "X = df.drop(columns=['label', 'subject'])\n",
    "y = df['label']\n",
    "groups = df['subject']  # <-- subject strings like 'sub-133'\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "accuracies = [] \n",
    "\n",
    "for train_idx, test_idx in logo.split(X, y, groups=groups):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    # Optional, scale inside loop to prevent leakage\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Simple classifier\n",
    "    model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(acc)\n",
    "\n",
    "print(f\"Mean Accuracy (LOSO): {sum(accuracies)/len(accuracies):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0945bff3-f688-40cf-916e-25bec692b547",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d763f416-6e82-4fbe-af19-44557c81dcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Mean Accuracy (LOSO with LDA): 0.2585\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# Features, label, and subject group\n",
    "X = df.drop(columns=['label', 'subject'])\n",
    "y = df['label']\n",
    "groups = df['subject'] \n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "accuracies = []\n",
    "\n",
    "for train_idx, test_idx in logo.split(X, y, groups=groups):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    # Scale inside loop to avoid leakage\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # LDA model\n",
    "    model = LinearDiscriminantAnalysis()\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(acc)\n",
    "\n",
    "print(f\"Mean Accuracy (LOSO with LDA): {sum(accuracies)/len(accuracies):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ece0409-ac75-4d27-bb62-596f36ae08ff",
   "metadata": {},
   "source": [
    "### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28ef728d-7c30-4b9d-88ca-54c29c7eec62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Mean Accuracy (LOSO with Random Forest): 0.2989\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Features, label, and subject group\n",
    "X = df.drop(columns=['label', 'subject'])\n",
    "y = df['label']\n",
    "groups = df['subject']  # <-- subject strings like 'sub-133'\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "accuracies = []\n",
    "\n",
    "for train_idx, test_idx in logo.split(X, y, groups=groups):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    # Optional: scale inside loop (not critical for RF, but harmless)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Random Forest Classifier\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(acc)\n",
    "\n",
    "print(f\"Mean Accuracy (LOSO with Random Forest): {sum(accuracies)/len(accuracies):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b124545-3547-4a29-b90e-03c8c7337c2b",
   "metadata": {},
   "source": [
    "### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56fc7852-4bfd-48f2-8b9c-3c2df995c5ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002336 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18360\n",
      "[LightGBM] [Info] Number of data points in the train set: 1143, number of used features: 72\n",
      "[LightGBM] [Info] Start training from score -1.388922\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002595 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18360\n",
      "[LightGBM] [Info] Number of data points in the train set: 1143, number of used features: 72\n",
      "[LightGBM] [Info] Start training from score -1.388922\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001811 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18360\n",
      "[LightGBM] [Info] Number of data points in the train set: 1143, number of used features: 72\n",
      "[LightGBM] [Info] Start training from score -1.388922\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001207 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18360\n",
      "[LightGBM] [Info] Number of data points in the train set: 1143, number of used features: 72\n",
      "[LightGBM] [Info] Start training from score -1.388922\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002521 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18360\n",
      "[LightGBM] [Info] Number of data points in the train set: 1143, number of used features: 72\n",
      "[LightGBM] [Info] Start training from score -1.388922\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003967 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18360\n",
      "[LightGBM] [Info] Number of data points in the train set: 1143, number of used features: 72\n",
      "[LightGBM] [Info] Start training from score -1.388922\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002144 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18360\n",
      "[LightGBM] [Info] Number of data points in the train set: 1143, number of used features: 72\n",
      "[LightGBM] [Info] Start training from score -1.388922\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001722 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18360\n",
      "[LightGBM] [Info] Number of data points in the train set: 1143, number of used features: 72\n",
      "[LightGBM] [Info] Start training from score -1.388922\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001224 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18360\n",
      "[LightGBM] [Info] Number of data points in the train set: 1143, number of used features: 72\n",
      "[LightGBM] [Info] Start training from score -1.388922\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002368 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18360\n",
      "[LightGBM] [Info] Number of data points in the train set: 1143, number of used features: 72\n",
      "[LightGBM] [Info] Start training from score -1.388922\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002526 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18360\n",
      "[LightGBM] [Info] Number of data points in the train set: 1143, number of used features: 72\n",
      "[LightGBM] [Info] Start training from score -1.388922\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001606 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18360\n",
      "[LightGBM] [Info] Number of data points in the train set: 1143, number of used features: 72\n",
      "[LightGBM] [Info] Start training from score -1.388922\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002482 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18360\n",
      "[LightGBM] [Info] Number of data points in the train set: 1143, number of used features: 72\n",
      "[LightGBM] [Info] Start training from score -1.388922\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002799 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18360\n",
      "[LightGBM] [Info] Number of data points in the train set: 1160, number of used features: 72\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001839 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18360\n",
      "[LightGBM] [Info] Number of data points in the train set: 1143, number of used features: 72\n",
      "[LightGBM] [Info] Start training from score -1.388922\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001096 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18360\n",
      "[LightGBM] [Info] Number of data points in the train set: 1143, number of used features: 72\n",
      "[LightGBM] [Info] Start training from score -1.388922\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003090 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18360\n",
      "[LightGBM] [Info] Number of data points in the train set: 1143, number of used features: 72\n",
      "[LightGBM] [Info] Start training from score -1.388922\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001927 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18360\n",
      "[LightGBM] [Info] Number of data points in the train set: 1143, number of used features: 72\n",
      "[LightGBM] [Info] Start training from score -1.388922\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002449 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18360\n",
      "[LightGBM] [Info] Number of data points in the train set: 1143, number of used features: 72\n",
      "[LightGBM] [Info] Start training from score -1.388922\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002536 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18360\n",
      "[LightGBM] [Info] Number of data points in the train set: 1143, number of used features: 72\n",
      "[LightGBM] [Info] Start training from score -1.388922\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001278 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18360\n",
      "[LightGBM] [Info] Number of data points in the train set: 1143, number of used features: 72\n",
      "[LightGBM] [Info] Start training from score -1.388922\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002036 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18360\n",
      "[LightGBM] [Info] Number of data points in the train set: 1143, number of used features: 72\n",
      "[LightGBM] [Info] Start training from score -1.388922\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003428 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18360\n",
      "[LightGBM] [Info] Number of data points in the train set: 1143, number of used features: 72\n",
      "[LightGBM] [Info] Start training from score -1.388922\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000913 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18360\n",
      "[LightGBM] [Info] Number of data points in the train set: 1143, number of used features: 72\n",
      "[LightGBM] [Info] Start training from score -1.388922\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001607 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18360\n",
      "[LightGBM] [Info] Number of data points in the train set: 1143, number of used features: 72\n",
      "[LightGBM] [Info] Start training from score -1.388922\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002163 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18360\n",
      "[LightGBM] [Info] Number of data points in the train set: 1143, number of used features: 72\n",
      "[LightGBM] [Info] Start training from score -1.388922\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001171 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18360\n",
      "[LightGBM] [Info] Number of data points in the train set: 1143, number of used features: 72\n",
      "[LightGBM] [Info] Start training from score -1.388922\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001336 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18360\n",
      "[LightGBM] [Info] Number of data points in the train set: 1143, number of used features: 72\n",
      "[LightGBM] [Info] Start training from score -1.388922\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001176 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18360\n",
      "[LightGBM] [Info] Number of data points in the train set: 1143, number of used features: 72\n",
      "[LightGBM] [Info] Start training from score -1.388922\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000884 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18360\n",
      "[LightGBM] [Info] Number of data points in the train set: 1143, number of used features: 72\n",
      "[LightGBM] [Info] Start training from score -1.388922\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "[LightGBM] [Info] Start training from score -1.385420\n",
      "✅ Mean Accuracy (LOSO with LGBM): 0.2610\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Features, label, and subject group\n",
    "X = df.drop(columns=['label', 'subject'])\n",
    "y = df['label']\n",
    "groups = df['subject'] \n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "accuracies = []\n",
    "\n",
    "for train_idx, test_idx in logo.split(X, y, groups=groups):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    # Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # LGBM Classifier\n",
    "    model = LGBMClassifier(random_state=42)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(acc)\n",
    "\n",
    "print(f\"Mean Accuracy (LOSO with LGBM): {sum(accuracies)/len(accuracies):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab0a0d1-3668-4653-a203-4fe6b078a7b8",
   "metadata": {},
   "source": [
    "#### LGBM Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f23a472-59d2-40f4-9c34-ca43509da84b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-28 16:26:11,648] A new study created in memory with name: no-name-8b9fc2b8-eed7-481b-b4c0-e8d179386df6\n",
      "[I 2025-07-28 16:27:07,512] Trial 0 finished with value: 0.25361150070126237 and parameters: {'n_estimators': 346, 'max_depth': 11, 'num_leaves': 43, 'learning_rate': 0.03560960604512054, 'feature_fraction': 0.8537768901900573, 'bagging_fraction': 0.6460194375053462, 'lambda_l1': 3.1174346573478506, 'lambda_l2': 3.529224410283165, 'min_child_samples': 32, 'max_bin': 167}. Best is trial 0 with value: 0.25361150070126237.\n",
      "[I 2025-07-28 16:29:16,429] Trial 1 finished with value: 0.2519985974754558 and parameters: {'n_estimators': 661, 'max_depth': 7, 'num_leaves': 54, 'learning_rate': 0.01775111955036761, 'feature_fraction': 0.6680715587737244, 'bagging_fraction': 0.8625961613006219, 'lambda_l1': 0.10862589866372785, 'lambda_l2': 4.099853884835902, 'min_child_samples': 22, 'max_bin': 140}. Best is trial 0 with value: 0.25361150070126237.\n",
      "[I 2025-07-28 16:29:42,140] Trial 2 finished with value: 0.26146563814866763 and parameters: {'n_estimators': 132, 'max_depth': 11, 'num_leaves': 114, 'learning_rate': 0.03124524101506234, 'feature_fraction': 0.8918155592851993, 'bagging_fraction': 0.8075936327617544, 'lambda_l1': 4.022112353594389, 'lambda_l2': 2.9406275032942353, 'min_child_samples': 34, 'max_bin': 152}. Best is trial 2 with value: 0.26146563814866763.\n",
      "[I 2025-07-28 16:33:14,255] Trial 3 finished with value: 0.25662692847124824 and parameters: {'n_estimators': 560, 'max_depth': 12, 'num_leaves': 120, 'learning_rate': 0.005743147488466565, 'feature_fraction': 0.6297697627172383, 'bagging_fraction': 0.7307836084146448, 'lambda_l1': 0.7293136927639271, 'lambda_l2': 1.833323193829166, 'min_child_samples': 17, 'max_bin': 183}. Best is trial 2 with value: 0.26146563814866763.\n",
      "[I 2025-07-28 16:34:50,462] Trial 4 finished with value: 0.2624824684431977 and parameters: {'n_estimators': 543, 'max_depth': 7, 'num_leaves': 49, 'learning_rate': 0.03531781190936096, 'feature_fraction': 0.6495486934571522, 'bagging_fraction': 0.8091646542111353, 'lambda_l1': 2.2200675583211598, 'lambda_l2': 4.852142063681488, 'min_child_samples': 26, 'max_bin': 133}. Best is trial 4 with value: 0.2624824684431977.\n",
      "[I 2025-07-28 16:38:19,209] Trial 5 finished with value: 0.25764375876577844 and parameters: {'n_estimators': 563, 'max_depth': 11, 'num_leaves': 105, 'learning_rate': 0.01748926251911788, 'feature_fraction': 0.7276497335806288, 'bagging_fraction': 0.5534926445454307, 'lambda_l1': 2.3621248600877394, 'lambda_l2': 3.4776744018768135, 'min_child_samples': 14, 'max_bin': 186}. Best is trial 4 with value: 0.2624824684431977.\n",
      "[I 2025-07-28 16:38:43,781] Trial 6 finished with value: 0.27717391304347827 and parameters: {'n_estimators': 240, 'max_depth': 4, 'num_leaves': 16, 'learning_rate': 0.004884148567496372, 'feature_fraction': 0.9994959647503499, 'bagging_fraction': 0.8316787071947063, 'lambda_l1': 3.735898701222183, 'lambda_l2': 3.8094376747428, 'min_child_samples': 97, 'max_bin': 118}. Best is trial 6 with value: 0.27717391304347827.\n",
      "[I 2025-07-28 16:40:03,340] Trial 7 finished with value: 0.2747545582047686 and parameters: {'n_estimators': 577, 'max_depth': 14, 'num_leaves': 100, 'learning_rate': 0.00514179647222179, 'feature_fraction': 0.7786157842129515, 'bagging_fraction': 0.9823823476390687, 'lambda_l1': 0.65667906369822, 'lambda_l2': 1.8198804978161731, 'min_child_samples': 72, 'max_bin': 137}. Best is trial 6 with value: 0.27717391304347827.\n",
      "[I 2025-07-28 16:40:46,438] Trial 8 finished with value: 0.26630434782608703 and parameters: {'n_estimators': 365, 'max_depth': 15, 'num_leaves': 24, 'learning_rate': 0.08181097481801494, 'feature_fraction': 0.8158302308180898, 'bagging_fraction': 0.7216425828794486, 'lambda_l1': 3.209421297940488, 'lambda_l2': 3.9311104023434633, 'min_child_samples': 82, 'max_bin': 230}. Best is trial 6 with value: 0.27717391304347827.\n",
      "[I 2025-07-28 16:42:08,101] Trial 9 finished with value: 0.24737026647966337 and parameters: {'n_estimators': 388, 'max_depth': 14, 'num_leaves': 95, 'learning_rate': 0.03307345588664834, 'feature_fraction': 0.8302494066600759, 'bagging_fraction': 0.9615968089118598, 'lambda_l1': 3.539256860883003, 'lambda_l2': 4.229087127578866, 'min_child_samples': 38, 'max_bin': 223}. Best is trial 6 with value: 0.27717391304347827.\n",
      "[I 2025-07-28 16:42:20,623] Trial 10 finished with value: 0.2805750350631137 and parameters: {'n_estimators': 176, 'max_depth': 3, 'num_leaves': 73, 'learning_rate': 0.0011009937318882663, 'feature_fraction': 0.9954290462094659, 'bagging_fraction': 0.8749283101653278, 'lambda_l1': 4.847199439984248, 'lambda_l2': 0.19139817807351545, 'min_child_samples': 98, 'max_bin': 77}. Best is trial 10 with value: 0.2805750350631137.\n",
      "[I 2025-07-28 16:42:33,996] Trial 11 finished with value: 0.2856241234221599 and parameters: {'n_estimators': 183, 'max_depth': 3, 'num_leaves': 75, 'learning_rate': 0.0011385104737101999, 'feature_fraction': 0.972668491616676, 'bagging_fraction': 0.8910038340126001, 'lambda_l1': 4.806209554592245, 'lambda_l2': 0.3328118056582504, 'min_child_samples': 98, 'max_bin': 73}. Best is trial 11 with value: 0.2856241234221599.\n",
      "[I 2025-07-28 16:42:43,920] Trial 12 finished with value: 0.2695301542776999 and parameters: {'n_estimators': 125, 'max_depth': 4, 'num_leaves': 78, 'learning_rate': 0.0013138831583042233, 'feature_fraction': 0.99796038152582, 'bagging_fraction': 0.8967390457772536, 'lambda_l1': 4.838763200599974, 'lambda_l2': 0.06361642179609162, 'min_child_samples': 99, 'max_bin': 65}. Best is trial 11 with value: 0.2856241234221599.\n",
      "[I 2025-07-28 16:42:59,967] Trial 13 finished with value: 0.2713183730715288 and parameters: {'n_estimators': 240, 'max_depth': 3, 'num_leaves': 76, 'learning_rate': 0.001014899265096128, 'feature_fraction': 0.9269894613757939, 'bagging_fraction': 0.923031351715176, 'lambda_l1': 4.987363784788697, 'lambda_l2': 0.023278897686077688, 'min_child_samples': 65, 'max_bin': 79}. Best is trial 11 with value: 0.2856241234221599.\n",
      "[I 2025-07-28 16:43:25,720] Trial 14 finished with value: 0.2904628330995792 and parameters: {'n_estimators': 236, 'max_depth': 6, 'num_leaves': 64, 'learning_rate': 0.002099388998117484, 'feature_fraction': 0.9394550767615384, 'bagging_fraction': 0.9046888218729755, 'lambda_l1': 4.334212215446064, 'lambda_l2': 0.93694134052135, 'min_child_samples': 84, 'max_bin': 100}. Best is trial 14 with value: 0.2904628330995792.\n",
      "[I 2025-07-28 16:43:58,008] Trial 15 finished with value: 0.28706171107994394 and parameters: {'n_estimators': 285, 'max_depth': 6, 'num_leaves': 63, 'learning_rate': 0.0024723270657160206, 'feature_fraction': 0.9274171844692868, 'bagging_fraction': 0.9951531915878932, 'lambda_l1': 4.166261447842941, 'lambda_l2': 1.1248521713065307, 'min_child_samples': 83, 'max_bin': 105}. Best is trial 14 with value: 0.2904628330995792.\n",
      "[I 2025-07-28 16:44:39,430] Trial 16 finished with value: 0.28625525946704067 and parameters: {'n_estimators': 293, 'max_depth': 7, 'num_leaves': 59, 'learning_rate': 0.0024890578332204738, 'feature_fraction': 0.9063463662283523, 'bagging_fraction': 0.9825614994285728, 'lambda_l1': 4.22198987933367, 'lambda_l2': 1.078655246891186, 'min_child_samples': 54, 'max_bin': 105}. Best is trial 14 with value: 0.2904628330995792.\n",
      "[I 2025-07-28 16:45:36,193] Trial 17 finished with value: 0.2842215988779804 and parameters: {'n_estimators': 461, 'max_depth': 6, 'num_leaves': 37, 'learning_rate': 0.002596435171568269, 'feature_fraction': 0.9398209359478895, 'bagging_fraction': 0.9370403695910294, 'lambda_l1': 1.7745096789956958, 'lambda_l2': 1.0425218201293758, 'min_child_samples': 82, 'max_bin': 103}. Best is trial 14 with value: 0.2904628330995792.\n",
      "[I 2025-07-28 16:47:13,659] Trial 18 finished with value: 0.2826086956521739 and parameters: {'n_estimators': 461, 'max_depth': 9, 'num_leaves': 89, 'learning_rate': 0.002475912248130966, 'feature_fraction': 0.8735619191975548, 'bagging_fraction': 0.6599969128506789, 'lambda_l1': 2.8266935039355454, 'lambda_l2': 0.983355474397702, 'min_child_samples': 49, 'max_bin': 98}. Best is trial 14 with value: 0.2904628330995792.\n",
      "[I 2025-07-28 16:47:49,249] Trial 19 finished with value: 0.26953015427769983 and parameters: {'n_estimators': 286, 'max_depth': 9, 'num_leaves': 58, 'learning_rate': 0.008856988440403599, 'feature_fraction': 0.768076143338539, 'bagging_fraction': 0.9870515028537497, 'lambda_l1': 4.1815245745496785, 'lambda_l2': 1.9157412795200144, 'min_child_samples': 86, 'max_bin': 119}. Best is trial 14 with value: 0.2904628330995792.\n",
      "[I 2025-07-28 16:48:50,620] Trial 20 finished with value: 0.28685133239831695 and parameters: {'n_estimators': 299, 'max_depth': 5, 'num_leaves': 35, 'learning_rate': 0.0019233110323699368, 'feature_fraction': 0.9471697781794861, 'bagging_fraction': 0.5145936715166906, 'lambda_l1': 1.6246035338187776, 'lambda_l2': 2.5950033530029684, 'min_child_samples': 66, 'max_bin': 253}. Best is trial 14 with value: 0.2904628330995792.\n",
      "[I 2025-07-28 16:49:34,986] Trial 21 finished with value: 0.2773842917251052 and parameters: {'n_estimators': 296, 'max_depth': 5, 'num_leaves': 34, 'learning_rate': 0.0019274678711072805, 'feature_fraction': 0.9483140518839465, 'bagging_fraction': 0.5043833389227756, 'lambda_l1': 1.5105706118261697, 'lambda_l2': 2.500864468428719, 'min_child_samples': 68, 'max_bin': 202}. Best is trial 14 with value: 0.2904628330995792.\n",
      "[I 2025-07-28 16:50:15,615] Trial 22 finished with value: 0.2646914446002805 and parameters: {'n_estimators': 234, 'max_depth': 6, 'num_leaves': 64, 'learning_rate': 0.003444028503784125, 'feature_fraction': 0.9068905751092352, 'bagging_fraction': 0.5810919443505802, 'lambda_l1': 1.7392865825688901, 'lambda_l2': 1.4692632805661228, 'min_child_samples': 78, 'max_bin': 245}. Best is trial 14 with value: 0.2904628330995792.\n",
      "[I 2025-07-28 16:51:07,969] Trial 23 finished with value: 0.2666900420757364 and parameters: {'n_estimators': 437, 'max_depth': 5, 'num_leaves': 85, 'learning_rate': 0.001895527924849338, 'feature_fraction': 0.9562072802944557, 'bagging_fraction': 0.7805086826931069, 'lambda_l1': 1.1520469587674005, 'lambda_l2': 2.552346496493417, 'min_child_samples': 59, 'max_bin': 89}. Best is trial 14 with value: 0.2904628330995792.\n",
      "[I 2025-07-28 16:51:54,635] Trial 24 finished with value: 0.28201262272089755 and parameters: {'n_estimators': 331, 'max_depth': 8, 'num_leaves': 29, 'learning_rate': 0.0033841878825853575, 'feature_fraction': 0.8588143565210462, 'bagging_fraction': 0.7008129651865964, 'lambda_l1': 4.511575105706834, 'lambda_l2': 0.6501656610025813, 'min_child_samples': 90, 'max_bin': 250}. Best is trial 14 with value: 0.2904628330995792.\n",
      "[I 2025-07-28 16:52:18,934] Trial 25 finished with value: 0.26931977559607295 and parameters: {'n_estimators': 192, 'max_depth': 5, 'num_leaves': 67, 'learning_rate': 0.001634163957630894, 'feature_fraction': 0.9125026238227334, 'bagging_fraction': 0.6079275268175549, 'lambda_l1': 3.5993174803864587, 'lambda_l2': 3.079206478046288, 'min_child_samples': 73, 'max_bin': 115}. Best is trial 14 with value: 0.2904628330995792.\n",
      "[I 2025-07-28 16:53:15,705] Trial 26 finished with value: 0.2771739130434783 and parameters: {'n_estimators': 413, 'max_depth': 6, 'num_leaves': 48, 'learning_rate': 0.006778934097148158, 'feature_fraction': 0.9616863358235946, 'bagging_fraction': 0.9399206592710537, 'lambda_l1': 2.6758690039748636, 'lambda_l2': 2.2407385578292467, 'min_child_samples': 89, 'max_bin': 161}. Best is trial 14 with value: 0.2904628330995792.\n",
      "[I 2025-07-28 16:54:06,435] Trial 27 finished with value: 0.28846423562412343 and parameters: {'n_estimators': 263, 'max_depth': 9, 'num_leaves': 42, 'learning_rate': 0.0033683861244136843, 'feature_fraction': 0.8817629048339845, 'bagging_fraction': 0.7705263466155872, 'lambda_l1': 2.027381021731055, 'lambda_l2': 1.5601156953733055, 'min_child_samples': 62, 'max_bin': 200}. Best is trial 14 with value: 0.2904628330995792.\n",
      "[I 2025-07-28 16:55:04,429] Trial 28 finished with value: 0.2812061711079944 and parameters: {'n_estimators': 253, 'max_depth': 9, 'num_leaves': 44, 'learning_rate': 0.003791311996966751, 'feature_fraction': 0.8822697384127647, 'bagging_fraction': 0.781974459957305, 'lambda_l1': 2.038424270413703, 'lambda_l2': 1.4560798930248773, 'min_child_samples': 44, 'max_bin': 211}. Best is trial 14 with value: 0.2904628330995792.\n",
      "[I 2025-07-28 16:55:19,538] Trial 29 finished with value: 0.2880434782608696 and parameters: {'n_estimators': 101, 'max_depth': 10, 'num_leaves': 51, 'learning_rate': 0.01323660481514919, 'feature_fraction': 0.8444647276887887, 'bagging_fraction': 0.6870937355612136, 'lambda_l1': 3.2262324967742897, 'lambda_l2': 0.5594248882321848, 'min_child_samples': 59, 'max_bin': 188}. Best is trial 14 with value: 0.2904628330995792.\n",
      "[I 2025-07-28 16:55:35,585] Trial 30 finished with value: 0.28464235624123424 and parameters: {'n_estimators': 102, 'max_depth': 10, 'num_leaves': 51, 'learning_rate': 0.015617300691292373, 'feature_fraction': 0.8470809465363568, 'bagging_fraction': 0.694035929531925, 'lambda_l1': 3.1008280785253324, 'lambda_l2': 0.596274049035401, 'min_child_samples': 58, 'max_bin': 178}. Best is trial 14 with value: 0.2904628330995792.\n",
      "[I 2025-07-28 16:55:53,758] Trial 31 finished with value: 0.2884642356241234 and parameters: {'n_estimators': 153, 'max_depth': 8, 'num_leaves': 64, 'learning_rate': 0.011636990303633783, 'feature_fraction': 0.8485735860940954, 'bagging_fraction': 0.6497995259567224, 'lambda_l1': 3.8570910239301677, 'lambda_l2': 1.4460724519703128, 'min_child_samples': 74, 'max_bin': 200}. Best is trial 14 with value: 0.2904628330995792.\n",
      "[I 2025-07-28 16:56:18,737] Trial 32 finished with value: 0.28604488078541374 and parameters: {'n_estimators': 162, 'max_depth': 8, 'num_leaves': 42, 'learning_rate': 0.010223127900289704, 'feature_fraction': 0.7854965853427004, 'bagging_fraction': 0.6358088364213891, 'lambda_l1': 3.1946025136293956, 'lambda_l2': 1.4537804386382829, 'min_child_samples': 63, 'max_bin': 202}. Best is trial 14 with value: 0.2904628330995792.\n",
      "[I 2025-07-28 16:56:35,479] Trial 33 finished with value: 0.27717391304347827 and parameters: {'n_estimators': 136, 'max_depth': 12, 'num_leaves': 55, 'learning_rate': 0.013797607159079161, 'feature_fraction': 0.745616440991039, 'bagging_fraction': 0.6572552392872947, 'lambda_l1': 3.8427223886439683, 'lambda_l2': 0.6364463717671316, 'min_child_samples': 75, 'max_bin': 171}. Best is trial 14 with value: 0.2904628330995792.\n",
      "[I 2025-07-28 16:57:10,218] Trial 34 finished with value: 0.29007713884992997 and parameters: {'n_estimators': 214, 'max_depth': 8, 'num_leaves': 69, 'learning_rate': 0.011533453768502698, 'feature_fraction': 0.8158681475901817, 'bagging_fraction': 0.7612462926617116, 'lambda_l1': 4.469593836733336, 'lambda_l2': 2.080250568226728, 'min_child_samples': 52, 'max_bin': 195}. Best is trial 14 with value: 0.2904628330995792.\n",
      "[I 2025-07-28 16:57:41,830] Trial 35 finished with value: 0.2886746143057503 and parameters: {'n_estimators': 206, 'max_depth': 8, 'num_leaves': 83, 'learning_rate': 0.022408480614019087, 'feature_fraction': 0.6990067648601884, 'bagging_fraction': 0.7566118020523255, 'lambda_l1': 4.5053618792301675, 'lambda_l2': 2.081851557468875, 'min_child_samples': 52, 'max_bin': 148}. Best is trial 14 with value: 0.2904628330995792.\n",
      "[I 2025-07-28 16:58:17,952] Trial 36 finished with value: 0.29109396914446006 and parameters: {'n_estimators': 216, 'max_depth': 10, 'num_leaves': 85, 'learning_rate': 0.007319794150216813, 'feature_fraction': 0.6797155984645206, 'bagging_fraction': 0.7568455954370265, 'lambda_l1': 4.524336577982989, 'lambda_l2': 2.13393934442508, 'min_child_samples': 44, 'max_bin': 148}. Best is trial 36 with value: 0.29109396914446006.\n",
      "[I 2025-07-28 16:58:52,533] Trial 37 finished with value: 0.2701262272089761 and parameters: {'n_estimators': 209, 'max_depth': 10, 'num_leaves': 84, 'learning_rate': 0.02322742180749615, 'feature_fraction': 0.693352781859197, 'bagging_fraction': 0.8322635608819825, 'lambda_l1': 4.501521136757487, 'lambda_l2': 2.0987039313379356, 'min_child_samples': 42, 'max_bin': 144}. Best is trial 36 with value: 0.29109396914446006.\n",
      "[I 2025-07-28 16:59:36,600] Trial 38 finished with value: 0.2638849929873773 and parameters: {'n_estimators': 206, 'max_depth': 11, 'num_leaves': 110, 'learning_rate': 0.04562540368652323, 'feature_fraction': 0.693595666633722, 'bagging_fraction': 0.7441739357587425, 'lambda_l1': 4.460342901130868, 'lambda_l2': 3.2149617267324717, 'min_child_samples': 28, 'max_bin': 152}. Best is trial 36 with value: 0.29109396914446006.\n",
      "[I 2025-07-28 17:00:24,657] Trial 39 finished with value: 0.27475455820476863 and parameters: {'n_estimators': 327, 'max_depth': 12, 'num_leaves': 93, 'learning_rate': 0.00798535703456564, 'feature_fraction': 0.6244510108466614, 'bagging_fraction': 0.8438019279457345, 'lambda_l1': 4.595688617264942, 'lambda_l2': 2.2900888463797777, 'min_child_samples': 52, 'max_bin': 124}. Best is trial 36 with value: 0.29109396914446006.\n",
      "[I 2025-07-28 17:02:08,490] Trial 40 finished with value: 0.2552244039270687 and parameters: {'n_estimators': 641, 'max_depth': 7, 'num_leaves': 82, 'learning_rate': 0.021578122639512285, 'feature_fraction': 0.6055036020420149, 'bagging_fraction': 0.7940540574102786, 'lambda_l1': 4.016459499778425, 'lambda_l2': 1.7968786535930523, 'min_child_samples': 47, 'max_bin': 149}. Best is trial 36 with value: 0.29109396914446006.\n",
      "[I 2025-07-28 17:02:48,795] Trial 41 finished with value: 0.24495091164095378 and parameters: {'n_estimators': 257, 'max_depth': 8, 'num_leaves': 99, 'learning_rate': 0.04726389888278474, 'feature_fraction': 0.6817750862378933, 'bagging_fraction': 0.7602269798175215, 'lambda_l1': 4.342589748909169, 'lambda_l2': 2.8097803184939925, 'min_child_samples': 36, 'max_bin': 164}. Best is trial 36 with value: 0.29109396914446006.\n",
      "[I 2025-07-28 17:03:32,089] Trial 42 finished with value: 0.27717391304347827 and parameters: {'n_estimators': 227, 'max_depth': 9, 'num_leaves': 69, 'learning_rate': 0.00453800663522541, 'feature_fraction': 0.7197256003395749, 'bagging_fraction': 0.7557054662872029, 'lambda_l1': 0.19259137527817893, 'lambda_l2': 1.7124096029844038, 'min_child_samples': 41, 'max_bin': 135}. Best is trial 36 with value: 0.29109396914446006.\n",
      "[I 2025-07-28 17:04:31,478] Trial 43 finished with value: 0.2844319775596073 and parameters: {'n_estimators': 266, 'max_depth': 10, 'num_leaves': 120, 'learning_rate': 0.006773166541522576, 'feature_fraction': 0.650982119257586, 'bagging_fraction': 0.7210012036165903, 'lambda_l1': 4.691204835732192, 'lambda_l2': 2.0918222998023754, 'min_child_samples': 31, 'max_bin': 220}. Best is trial 36 with value: 0.29109396914446006.\n",
      "[I 2025-07-28 17:05:38,813] Trial 44 finished with value: 0.2622720897615708 and parameters: {'n_estimators': 363, 'max_depth': 7, 'num_leaves': 69, 'learning_rate': 0.02374298927724285, 'feature_fraction': 0.8129631487181483, 'bagging_fraction': 0.8137680756850524, 'lambda_l1': 2.1769236485738075, 'lambda_l2': 1.6983178806903427, 'min_child_samples': 48, 'max_bin': 194}. Best is trial 36 with value: 0.29109396914446006.\n",
      "[I 2025-07-28 17:06:52,410] Trial 45 finished with value: 0.2701262272089762 and parameters: {'n_estimators': 219, 'max_depth': 11, 'num_leaves': 79, 'learning_rate': 0.018478950783220368, 'feature_fraction': 0.7173323176059383, 'bagging_fraction': 0.8666483256743832, 'lambda_l1': 3.537842704745676, 'lambda_l2': 1.240586329060037, 'min_child_samples': 17, 'max_bin': 173}. Best is trial 36 with value: 0.29109396914446006.\n",
      "[I 2025-07-28 17:07:19,548] Trial 46 finished with value: 0.28786816269284715 and parameters: {'n_estimators': 181, 'max_depth': 8, 'num_leaves': 22, 'learning_rate': 0.0063720501216431, 'feature_fraction': 0.7545709941912977, 'bagging_fraction': 0.765623054787214, 'lambda_l1': 4.972264551393559, 'lambda_l2': 2.794975790116946, 'min_child_samples': 53, 'max_bin': 128}. Best is trial 36 with value: 0.29109396914446006.\n",
      "[I 2025-07-28 17:07:40,851] Trial 47 finished with value: 0.27798036465638154 and parameters: {'n_estimators': 152, 'max_depth': 11, 'num_leaves': 89, 'learning_rate': 0.00431129920535029, 'feature_fraction': 0.6589295238660743, 'bagging_fraction': 0.7320765747783928, 'lambda_l1': 4.028840671102076, 'lambda_l2': 2.0333241514317164, 'min_child_samples': 61, 'max_bin': 156}. Best is trial 36 with value: 0.29109396914446006.\n",
      "[I 2025-07-28 17:08:29,676] Trial 48 finished with value: 0.25904628330995794 and parameters: {'n_estimators': 315, 'max_depth': 9, 'num_leaves': 106, 'learning_rate': 0.0919085030324809, 'feature_fraction': 0.8039630175846657, 'bagging_fraction': 0.8051389896174301, 'lambda_l1': 2.44839524317542, 'lambda_l2': 2.277925079975883, 'min_child_samples': 38, 'max_bin': 229}. Best is trial 36 with value: 0.29109396914446006.\n",
      "[I 2025-07-28 17:09:21,481] Trial 49 finished with value: 0.28625525946704067 and parameters: {'n_estimators': 265, 'max_depth': 7, 'num_leaves': 73, 'learning_rate': 0.028104880282954006, 'feature_fraction': 0.8253908839434387, 'bagging_fraction': 0.7117427432357479, 'lambda_l1': 1.3479769975897022, 'lambda_l2': 0.8888468422282056, 'min_child_samples': 56, 'max_bin': 182}. Best is trial 36 with value: 0.29109396914446006.\n",
      "[I 2025-07-28 17:10:04,073] Trial 50 finished with value: 0.2795932678821879 and parameters: {'n_estimators': 351, 'max_depth': 12, 'num_leaves': 90, 'learning_rate': 0.008911235857168312, 'feature_fraction': 0.9801766917078705, 'bagging_fraction': 0.8491848087423668, 'lambda_l1': 4.7257815160427405, 'lambda_l2': 4.6357034165698705, 'min_child_samples': 94, 'max_bin': 210}. Best is trial 36 with value: 0.29109396914446006.\n",
      "[I 2025-07-28 17:10:22,368] Trial 51 finished with value: 0.27878681626928475 and parameters: {'n_estimators': 148, 'max_depth': 8, 'num_leaves': 61, 'learning_rate': 0.011944260220764483, 'feature_fraction': 0.8719140933279826, 'bagging_fraction': 0.7356736262942625, 'lambda_l1': 4.323922752369706, 'lambda_l2': 1.566261743907671, 'min_child_samples': 70, 'max_bin': 192}. Best is trial 36 with value: 0.29109396914446006.\n",
      "[I 2025-07-28 17:10:47,433] Trial 52 finished with value: 0.2773842917251052 and parameters: {'n_estimators': 197, 'max_depth': 8, 'num_leaves': 79, 'learning_rate': 0.011099292788175917, 'feature_fraction': 0.8938528552985788, 'bagging_fraction': 0.6710517887827323, 'lambda_l1': 3.8530701335903412, 'lambda_l2': 1.2895499232716037, 'min_child_samples': 79, 'max_bin': 200}. Best is trial 36 with value: 0.29109396914446006.\n",
      "[I 2025-07-28 17:11:17,596] Trial 53 finished with value: 0.283625525946704 and parameters: {'n_estimators': 182, 'max_depth': 9, 'num_leaves': 65, 'learning_rate': 0.002954881396405897, 'feature_fraction': 0.7843757227375596, 'bagging_fraction': 0.6226731476156817, 'lambda_l1': 3.98971348926095, 'lambda_l2': 0.8228734560228161, 'min_child_samples': 52, 'max_bin': 213}. Best is trial 36 with value: 0.29109396914446006.\n",
      "[I 2025-07-28 17:11:43,359] Trial 54 finished with value: 0.2955119214586255 and parameters: {'n_estimators': 124, 'max_depth': 10, 'num_leaves': 126, 'learning_rate': 0.005771047465437344, 'feature_fraction': 0.8311723818244195, 'bagging_fraction': 0.8952820145823789, 'lambda_l1': 4.36905073404463, 'lambda_l2': 1.9143532826153988, 'min_child_samples': 45, 'max_bin': 237}. Best is trial 54 with value: 0.2955119214586255.\n",
      "[I 2025-07-28 17:12:05,790] Trial 55 finished with value: 0.2908835904628331 and parameters: {'n_estimators': 116, 'max_depth': 10, 'num_leaves': 98, 'learning_rate': 0.004938157144399731, 'feature_fraction': 0.8288410172860176, 'bagging_fraction': 0.9059517966460883, 'lambda_l1': 4.313074110423992, 'lambda_l2': 1.9375903853822054, 'min_child_samples': 45, 'max_bin': 236}. Best is trial 54 with value: 0.2955119214586255.\n",
      "[I 2025-07-28 17:12:34,701] Trial 56 finished with value: 0.29249649368863956 and parameters: {'n_estimators': 125, 'max_depth': 13, 'num_leaves': 118, 'learning_rate': 0.005630201308929503, 'feature_fraction': 0.7342546700075752, 'bagging_fraction': 0.957719408080601, 'lambda_l1': 4.334684525161737, 'lambda_l2': 2.4195726583778168, 'min_child_samples': 45, 'max_bin': 236}. Best is trial 54 with value: 0.2955119214586255.\n",
      "[I 2025-07-28 17:13:00,303] Trial 57 finished with value: 0.29410939691444604 and parameters: {'n_estimators': 113, 'max_depth': 13, 'num_leaves': 127, 'learning_rate': 0.006075364130955716, 'feature_fraction': 0.7537705007014949, 'bagging_fraction': 0.9183360218812221, 'lambda_l1': 4.216557399742165, 'lambda_l2': 2.406330372698153, 'min_child_samples': 45, 'max_bin': 240}. Best is trial 54 with value: 0.2955119214586255.\n",
      "[I 2025-07-28 17:13:32,879] Trial 58 finished with value: 0.2876577840112202 and parameters: {'n_estimators': 120, 'max_depth': 13, 'num_leaves': 127, 'learning_rate': 0.005000715957646631, 'feature_fraction': 0.7362838140028144, 'bagging_fraction': 0.9049908316041541, 'lambda_l1': 4.19962975992825, 'lambda_l2': 3.413595111534474, 'min_child_samples': 33, 'max_bin': 241}. Best is trial 54 with value: 0.2955119214586255.\n",
      "[I 2025-07-28 17:13:59,283] Trial 59 finished with value: 0.28341514726507716 and parameters: {'n_estimators': 122, 'max_depth': 15, 'num_leaves': 126, 'learning_rate': 0.007744482643023814, 'feature_fraction': 0.765442172441819, 'bagging_fraction': 0.9623842073534983, 'lambda_l1': 3.682651988507137, 'lambda_l2': 2.391161566966602, 'min_child_samples': 44, 'max_bin': 243}. Best is trial 54 with value: 0.2955119214586255.\n",
      "[I 2025-07-28 17:14:53,956] Trial 60 finished with value: 0.28302945301542776 and parameters: {'n_estimators': 169, 'max_depth': 13, 'num_leaves': 118, 'learning_rate': 0.005590875237793622, 'feature_fraction': 0.6758001229373846, 'bagging_fraction': 0.9184962764022838, 'lambda_l1': 3.459820701132342, 'lambda_l2': 2.692725602949051, 'min_child_samples': 24, 'max_bin': 236}. Best is trial 54 with value: 0.2955119214586255.\n",
      "[I 2025-07-28 17:15:31,634] Trial 61 finished with value: 0.2860448807854137 and parameters: {'n_estimators': 131, 'max_depth': 14, 'num_leaves': 114, 'learning_rate': 0.005820283016409286, 'feature_fraction': 0.8291842401962878, 'bagging_fraction': 0.9611557577380199, 'lambda_l1': 4.286122229040095, 'lambda_l2': 3.003191963285373, 'min_child_samples': 39, 'max_bin': 233}. Best is trial 54 with value: 0.2955119214586255.\n",
      "[I 2025-07-28 17:15:54,282] Trial 62 finished with value: 0.2842215988779804 and parameters: {'n_estimators': 110, 'max_depth': 13, 'num_leaves': 125, 'learning_rate': 0.004122921347786476, 'feature_fraction': 0.7953561901261954, 'bagging_fraction': 0.8843695527334376, 'lambda_l1': 4.816288854352884, 'lambda_l2': 2.3766069599454296, 'min_child_samples': 46, 'max_bin': 224}. Best is trial 54 with value: 0.2955119214586255.\n",
      "[I 2025-07-28 17:17:55,379] Trial 63 finished with value: 0.2717391304347826 and parameters: {'n_estimators': 520, 'max_depth': 14, 'num_leaves': 122, 'learning_rate': 0.007744458936134764, 'feature_fraction': 0.7646296410091238, 'bagging_fraction': 0.9410373457527612, 'lambda_l1': 4.090186451816561, 'lambda_l2': 2.636997604681792, 'min_child_samples': 43, 'max_bin': 254}. Best is trial 54 with value: 0.2955119214586255.\n",
      "[I 2025-07-28 17:18:23,609] Trial 64 finished with value: 0.289866760168303 and parameters: {'n_estimators': 164, 'max_depth': 11, 'num_leaves': 97, 'learning_rate': 0.0053105033408914456, 'feature_fraction': 0.7097796195732164, 'bagging_fraction': 0.9060655392623204, 'lambda_l1': 4.322590017119243, 'lambda_l2': 1.9381572368946673, 'min_child_samples': 49, 'max_bin': 239}. Best is trial 54 with value: 0.2955119214586255.\n",
      "[I 2025-07-28 17:18:56,171] Trial 65 finished with value: 0.2715287517531557 and parameters: {'n_estimators': 141, 'max_depth': 12, 'num_leaves': 115, 'learning_rate': 0.00909730504377189, 'feature_fraction': 0.8082865734098666, 'bagging_fraction': 0.9698974266583826, 'lambda_l1': 4.682827948161522, 'lambda_l2': 1.8998035160260658, 'min_child_samples': 36, 'max_bin': 219}. Best is trial 54 with value: 0.2955119214586255.\n",
      "[I 2025-07-28 17:19:12,339] Trial 66 finished with value: 0.28948106591865364 and parameters: {'n_estimators': 117, 'max_depth': 4, 'num_leaves': 111, 'learning_rate': 0.00717552356586518, 'feature_fraction': 0.7338452692518973, 'bagging_fraction': 0.9247369050158093, 'lambda_l1': 4.925283788501751, 'lambda_l2': 2.478019675327016, 'min_child_samples': 56, 'max_bin': 247}. Best is trial 54 with value: 0.2955119214586255.\n",
      "[I 2025-07-28 17:19:34,515] Trial 67 finished with value: 0.27496493688639545 and parameters: {'n_estimators': 102, 'max_depth': 10, 'num_leaves': 122, 'learning_rate': 0.0028023856321679965, 'feature_fraction': 0.7956143347110777, 'bagging_fraction': 0.9991910979692704, 'lambda_l1': 3.3832362977987454, 'lambda_l2': 2.1860001977819246, 'min_child_samples': 40, 'max_bin': 226}. Best is trial 54 with value: 0.2955119214586255.\n",
      "[I 2025-07-28 17:20:24,914] Trial 68 finished with value: 0.2860448807854138 and parameters: {'n_estimators': 180, 'max_depth': 13, 'num_leaves': 105, 'learning_rate': 0.006039671526951587, 'feature_fraction': 0.7461365753159815, 'bagging_fraction': 0.8838228084893489, 'lambda_l1': 4.415350852781549, 'lambda_l2': 0.39317202039197663, 'min_child_samples': 30, 'max_bin': 232}. Best is trial 54 with value: 0.2955119214586255.\n",
      "[I 2025-07-28 17:20:58,488] Trial 69 finished with value: 0.27356241234221595 and parameters: {'n_estimators': 239, 'max_depth': 10, 'num_leaves': 101, 'learning_rate': 0.0013824246554085937, 'feature_fraction': 0.8258146706924436, 'bagging_fraction': 0.9440948755127501, 'lambda_l1': 4.61506988667952, 'lambda_l2': 3.2228233873419256, 'min_child_samples': 50, 'max_bin': 91}. Best is trial 54 with value: 0.2955119214586255.\n",
      "[I 2025-07-28 17:21:30,492] Trial 70 finished with value: 0.29631837307152875 and parameters: {'n_estimators': 146, 'max_depth': 15, 'num_leaves': 118, 'learning_rate': 0.003682868831708747, 'feature_fraction': 0.8601044304681416, 'bagging_fraction': 0.8693474700743267, 'lambda_l1': 3.843390601332877, 'lambda_l2': 2.8227380569444622, 'min_child_samples': 45, 'max_bin': 218}. Best is trial 70 with value: 0.29631837307152875.\n",
      "[I 2025-07-28 17:21:58,803] Trial 71 finished with value: 0.28804347826086957 and parameters: {'n_estimators': 139, 'max_depth': 15, 'num_leaves': 117, 'learning_rate': 0.0021852088870671594, 'feature_fraction': 0.8600176150772098, 'bagging_fraction': 0.863050943408418, 'lambda_l1': 4.160231162786708, 'lambda_l2': 2.8020020997902404, 'min_child_samples': 45, 'max_bin': 216}. Best is trial 70 with value: 0.29631837307152875.\n",
      "[I 2025-07-28 17:23:01,336] Trial 72 finished with value: 0.2884642356241234 and parameters: {'n_estimators': 155, 'max_depth': 14, 'num_leaves': 128, 'learning_rate': 0.003696189126689812, 'feature_fraction': 0.8401579995103168, 'bagging_fraction': 0.8985815688269408, 'lambda_l1': 3.7618220328435577, 'lambda_l2': 2.4385591832907916, 'min_child_samples': 36, 'max_bin': 207}. Best is trial 70 with value: 0.29631837307152875.\n",
      "[I 2025-07-28 17:23:44,557] Trial 73 finished with value: 0.2870617110799439 and parameters: {'n_estimators': 130, 'max_depth': 11, 'num_leaves': 110, 'learning_rate': 0.004500206371878764, 'feature_fraction': 0.9240903142862654, 'bagging_fraction': 0.9169052893667526, 'lambda_l1': 3.974292033048502, 'lambda_l2': 3.8018018978016244, 'min_child_samples': 50, 'max_bin': 236}. Best is trial 70 with value: 0.29631837307152875.\n",
      "[I 2025-07-28 17:24:20,559] Trial 74 finished with value: 0.287061711079944 and parameters: {'n_estimators': 166, 'max_depth': 6, 'num_leaves': 105, 'learning_rate': 0.005067774504025778, 'feature_fraction': 0.7725146029717217, 'bagging_fraction': 0.8757999871201549, 'lambda_l1': 4.583217241642181, 'lambda_l2': 2.936157341705831, 'min_child_samples': 45, 'max_bin': 226}. Best is trial 70 with value: 0.29631837307152875.\n",
      "[I 2025-07-28 17:25:15,458] Trial 75 finished with value: 0.2949158485273493 and parameters: {'n_estimators': 189, 'max_depth': 15, 'num_leaves': 123, 'learning_rate': 0.0038836859779799943, 'feature_fraction': 0.8187990177612811, 'bagging_fraction': 0.9292547003821863, 'lambda_l1': 4.447370997853169, 'lambda_l2': 1.692342700155113, 'min_child_samples': 42, 'max_bin': 244}. Best is trial 70 with value: 0.29631837307152875.\n",
      "[I 2025-07-28 17:26:07,535] Trial 76 finished with value: 0.2973352033660589 and parameters: {'n_estimators': 186, 'max_depth': 15, 'num_leaves': 123, 'learning_rate': 0.0029495256886591125, 'feature_fraction': 0.8641377363167525, 'bagging_fraction': 0.9299699819188052, 'lambda_l1': 4.80878743141239, 'lambda_l2': 1.7456469828290204, 'min_child_samples': 41, 'max_bin': 249}. Best is trial 76 with value: 0.2973352033660589.\n",
      "[I 2025-07-28 17:26:50,488] Trial 77 finished with value: 0.2941093969144461 and parameters: {'n_estimators': 197, 'max_depth': 15, 'num_leaves': 123, 'learning_rate': 0.0032050603176680377, 'feature_fraction': 0.8674690696594105, 'bagging_fraction': 0.9509973706886901, 'lambda_l1': 4.8283561525097785, 'lambda_l2': 1.6515776833036337, 'min_child_samples': 42, 'max_bin': 248}. Best is trial 76 with value: 0.2973352033660589.\n",
      "[I 2025-07-28 17:27:39,692] Trial 78 finished with value: 0.28604488078541374 and parameters: {'n_estimators': 190, 'max_depth': 15, 'num_leaves': 123, 'learning_rate': 0.003048417453731498, 'feature_fraction': 0.8601464094536712, 'bagging_fraction': 0.976109500148847, 'lambda_l1': 4.812191367051563, 'lambda_l2': 1.6759116132606566, 'min_child_samples': 34, 'max_bin': 250}. Best is trial 76 with value: 0.2973352033660589.\n",
      "[I 2025-07-28 17:28:23,652] Trial 79 finished with value: 0.2924964936886396 and parameters: {'n_estimators': 191, 'max_depth': 15, 'num_leaves': 120, 'learning_rate': 0.0036881140616077143, 'feature_fraction': 0.8925274053817501, 'bagging_fraction': 0.9573524529415137, 'lambda_l1': 4.997722855137287, 'lambda_l2': 1.3088266412312617, 'min_child_samples': 41, 'max_bin': 245}. Best is trial 76 with value: 0.2973352033660589.\n",
      "[I 2025-07-28 17:31:22,563] Trial 80 finished with value: 0.27314165497896215 and parameters: {'n_estimators': 684, 'max_depth': 15, 'num_leaves': 119, 'learning_rate': 0.003965926062042814, 'feature_fraction': 0.8947785850925719, 'bagging_fraction': 0.9509508083532769, 'lambda_l1': 4.987326955192891, 'lambda_l2': 1.256309981969964, 'min_child_samples': 41, 'max_bin': 255}. Best is trial 76 with value: 0.2973352033660589.\n",
      "[I 2025-07-28 17:32:07,219] Trial 81 finished with value: 0.2965287517531557 and parameters: {'n_estimators': 171, 'max_depth': 14, 'num_leaves': 124, 'learning_rate': 0.0033375316423076714, 'feature_fraction': 0.8697813150650968, 'bagging_fraction': 0.9311036841359678, 'lambda_l1': 4.737724706993434, 'lambda_l2': 1.7925154474400424, 'min_child_samples': 42, 'max_bin': 245}. Best is trial 76 with value: 0.2973352033660589.\n",
      "[I 2025-07-28 17:32:50,867] Trial 82 finished with value: 0.2892706872370267 and parameters: {'n_estimators': 196, 'max_depth': 14, 'num_leaves': 124, 'learning_rate': 0.0030891207043081733, 'feature_fraction': 0.8870395074626042, 'bagging_fraction': 0.9319221316294356, 'lambda_l1': 4.733113213581583, 'lambda_l2': 1.5883915511924769, 'min_child_samples': 38, 'max_bin': 247}. Best is trial 76 with value: 0.2973352033660589.\n",
      "[I 2025-07-28 17:33:21,044] Trial 83 finished with value: 0.2957223001402524 and parameters: {'n_estimators': 146, 'max_depth': 15, 'num_leaves': 113, 'learning_rate': 0.0024167396765866914, 'feature_fraction': 0.8750661148280764, 'bagging_fraction': 0.9513936890622314, 'lambda_l1': 4.86359387100786, 'lambda_l2': 1.3922917359264997, 'min_child_samples': 42, 'max_bin': 243}. Best is trial 76 with value: 0.2973352033660589.\n",
      "[I 2025-07-28 17:33:55,438] Trial 84 finished with value: 0.29572230014025247 and parameters: {'n_estimators': 167, 'max_depth': 15, 'num_leaves': 128, 'learning_rate': 0.002435044385366912, 'feature_fraction': 0.8706262591876349, 'bagging_fraction': 0.9833652384664592, 'lambda_l1': 4.877358865857774, 'lambda_l2': 1.3574277334424396, 'min_child_samples': 42, 'max_bin': 244}. Best is trial 76 with value: 0.2973352033660589.\n",
      "[I 2025-07-28 17:34:23,972] Trial 85 finished with value: 0.29169004207573634 and parameters: {'n_estimators': 147, 'max_depth': 14, 'num_leaves': 128, 'learning_rate': 0.002292583953046219, 'feature_fraction': 0.8702567432277503, 'bagging_fraction': 0.9822790610869137, 'lambda_l1': 4.890363254991632, 'lambda_l2': 1.164883131005957, 'min_child_samples': 48, 'max_bin': 249}. Best is trial 76 with value: 0.2973352033660589.\n",
      "[I 2025-07-28 17:35:04,717] Trial 86 finished with value: 0.2807854137447405 and parameters: {'n_estimators': 168, 'max_depth': 15, 'num_leaves': 112, 'learning_rate': 0.001732128499380368, 'feature_fraction': 0.8407186042335666, 'bagging_fraction': 0.9348899634812297, 'lambda_l1': 4.785306457032143, 'lambda_l2': 1.7949280971796113, 'min_child_samples': 36, 'max_bin': 241}. Best is trial 76 with value: 0.2973352033660589.\n",
      "[I 2025-07-28 17:35:54,394] Trial 87 finished with value: 0.28544880785413745 and parameters: {'n_estimators': 174, 'max_depth': 15, 'num_leaves': 116, 'learning_rate': 0.0025279123564195005, 'feature_fraction': 0.8674205440991485, 'bagging_fraction': 0.9230301863150098, 'lambda_l1': 4.6241498080140175, 'lambda_l2': 1.4364695137139736, 'min_child_samples': 28, 'max_bin': 230}. Best is trial 76 with value: 0.2973352033660589.\n",
      "[I 2025-07-28 17:36:48,117] Trial 88 finished with value: 0.2933029453015428 and parameters: {'n_estimators': 226, 'max_depth': 14, 'num_leaves': 122, 'learning_rate': 0.002706907619949859, 'feature_fraction': 0.8523964784578049, 'bagging_fraction': 0.9932450672265642, 'lambda_l1': 4.454346163364848, 'lambda_l2': 1.4019916481475196, 'min_child_samples': 42, 'max_bin': 251}. Best is trial 76 with value: 0.2973352033660589.\n",
      "[I 2025-07-28 17:37:29,234] Trial 89 finished with value: 0.29007713884992986 and parameters: {'n_estimators': 142, 'max_depth': 15, 'num_leaves': 108, 'learning_rate': 0.003275176640326844, 'feature_fraction': 0.9073557421543132, 'bagging_fraction': 0.9702612636811614, 'lambda_l1': 4.841857980415953, 'lambda_l2': 1.0950557201148203, 'min_child_samples': 34, 'max_bin': 244}. Best is trial 76 with value: 0.2973352033660589.\n",
      "[I 2025-07-28 17:38:37,850] Trial 90 finished with value: 0.2828190743338009 and parameters: {'n_estimators': 248, 'max_depth': 15, 'num_leaves': 125, 'learning_rate': 0.001603715925880616, 'feature_fraction': 0.8779176932227526, 'bagging_fraction': 0.9480006416978468, 'lambda_l1': 4.697936540091307, 'lambda_l2': 1.617504219989597, 'min_child_samples': 38, 'max_bin': 255}. Best is trial 76 with value: 0.2973352033660589.\n",
      "[I 2025-07-28 17:39:36,834] Trial 91 finished with value: 0.28765778401122016 and parameters: {'n_estimators': 222, 'max_depth': 14, 'num_leaves': 122, 'learning_rate': 0.0027009209790525954, 'feature_fraction': 0.8542608783458704, 'bagging_fraction': 0.9890140832867944, 'lambda_l1': 4.503890857886759, 'lambda_l2': 1.4051667062048154, 'min_child_samples': 42, 'max_bin': 252}. Best is trial 76 with value: 0.2973352033660589.\n",
      "[I 2025-07-28 17:40:26,249] Trial 92 finished with value: 0.2933029453015428 and parameters: {'n_estimators': 206, 'max_depth': 14, 'num_leaves': 113, 'learning_rate': 0.0020706776629499024, 'feature_fraction': 0.8379058845909948, 'bagging_fraction': 0.9131377828803755, 'lambda_l1': 4.392470076628004, 'lambda_l2': 1.756287878686761, 'min_child_samples': 47, 'max_bin': 239}. Best is trial 76 with value: 0.2973352033660589.\n",
      "[I 2025-07-28 17:40:59,189] Trial 93 finished with value: 0.3110448807854137 and parameters: {'n_estimators': 158, 'max_depth': 14, 'num_leaves': 120, 'learning_rate': 0.0023129559233807115, 'feature_fraction': 0.8181304692531582, 'bagging_fraction': 0.9302593262691019, 'lambda_l1': 4.583363901282219, 'lambda_l2': 1.9891249768767543, 'min_child_samples': 43, 'max_bin': 248}. Best is trial 93 with value: 0.3110448807854137.\n",
      "[I 2025-07-28 17:41:34,309] Trial 94 finished with value: 0.2916900420757363 and parameters: {'n_estimators': 158, 'max_depth': 14, 'num_leaves': 120, 'learning_rate': 0.0034339789675066584, 'feature_fraction': 0.8216461355892262, 'bagging_fraction': 0.9322572422348886, 'lambda_l1': 4.878913461320201, 'lambda_l2': 2.0021186744284964, 'min_child_samples': 39, 'max_bin': 244}. Best is trial 93 with value: 0.3110448807854137.\n",
      "[I 2025-07-28 17:42:00,999] Trial 95 finished with value: 0.2989481065918653 and parameters: {'n_estimators': 135, 'max_depth': 15, 'num_leaves': 125, 'learning_rate': 0.0023419310388895333, 'feature_fraction': 0.862034680435294, 'bagging_fraction': 0.8890952079368835, 'lambda_l1': 4.589907021427502, 'lambda_l2': 1.8637620593000062, 'min_child_samples': 43, 'max_bin': 233}. Best is trial 93 with value: 0.3110448807854137.\n",
      "[I 2025-07-28 17:42:48,772] Trial 96 finished with value: 0.27899719495091163 and parameters: {'n_estimators': 181, 'max_depth': 15, 'num_leaves': 116, 'learning_rate': 0.0018294503608310704, 'feature_fraction': 0.8633622295117892, 'bagging_fraction': 0.8953878550097967, 'lambda_l1': 4.617066158430779, 'lambda_l2': 1.5532672532205523, 'min_child_samples': 31, 'max_bin': 228}. Best is trial 93 with value: 0.3110448807854137.\n",
      "[I 2025-07-28 17:43:20,178] Trial 97 finished with value: 0.28884992987377284 and parameters: {'n_estimators': 155, 'max_depth': 15, 'num_leaves': 124, 'learning_rate': 0.002290057321378563, 'feature_fraction': 0.9167626697974584, 'bagging_fraction': 0.8558104164403857, 'lambda_l1': 4.757384194944624, 'lambda_l2': 1.8424766762450813, 'min_child_samples': 50, 'max_bin': 233}. Best is trial 93 with value: 0.3110448807854137.\n",
      "[I 2025-07-28 17:43:50,208] Trial 98 finished with value: 0.2715287517531556 and parameters: {'n_estimators': 134, 'max_depth': 15, 'num_leaves': 126, 'learning_rate': 0.0013756032575422755, 'feature_fraction': 0.835523235148712, 'bagging_fraction': 0.8812193330016805, 'lambda_l1': 2.717716767296496, 'lambda_l2': 2.2226778582578177, 'min_child_samples': 43, 'max_bin': 222}. Best is trial 93 with value: 0.3110448807854137.\n",
      "[I 2025-07-28 17:44:38,112] Trial 99 finished with value: 0.2981416549789621 and parameters: {'n_estimators': 203, 'max_depth': 14, 'num_leaves': 120, 'learning_rate': 0.0015339258940911134, 'feature_fraction': 0.8481105563664371, 'bagging_fraction': 0.9513232592289387, 'lambda_l1': 4.54194682791503, 'lambda_l2': 1.690758258535177, 'min_child_samples': 47, 'max_bin': 248}. Best is trial 93 with value: 0.3110448807854137.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best Hyperparameters:\n",
      "{'n_estimators': 158, 'max_depth': 14, 'num_leaves': 120, 'learning_rate': 0.0023129559233807115, 'feature_fraction': 0.8181304692531582, 'bagging_fraction': 0.9302593262691019, 'lambda_l1': 4.583363901282219, 'lambda_l2': 1.9891249768767543, 'min_child_samples': 43, 'max_bin': 248}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import LeaveOneGroupOut, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "X = df.drop(columns=['subject', 'label'])\n",
    "y = df['label']\n",
    "groups = df['subject']\n",
    "\n",
    "# Define Optuna Objective\n",
    "def objective(trial):\n",
    "    # Suggested hyperparameters\n",
    "    params = {\n",
    "        'objective': 'multiclass',\n",
    "        'num_class': 4, \n",
    "        'metric': 'multi_logloss',\n",
    "        'verbosity': -1,\n",
    "        'n_jobs': -1,\n",
    "\n",
    "        # parameters that will be tuned\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 700),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 16, 128),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.1, log=True),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 5.0),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 5.0),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),\n",
    "        'max_bin': trial.suggest_int('max_bin', 64, 255),\n",
    "    }\n",
    "\n",
    "\n",
    "    # Pipeline with scaling + LGBM\n",
    "    clf = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        LGBMClassifier(**params)\n",
    "    )\n",
    "\n",
    "    logo = LeaveOneGroupOut()\n",
    "    scores = cross_val_score(clf, X, y, cv=logo.split(X, y, groups=groups), scoring=\"accuracy\")\n",
    "    return scores.mean()\n",
    "\n",
    "# Run Optimization\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Show best hyperparameters\n",
    "print(\"✅ Best Hyperparameters:\")\n",
    "print(study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bc1d1cf-e91a-47a7-b355-6a26765f35cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Test Subject: sub-101 | 🟢 Train Acc: 0.66 | 🔵 Test Acc: 0.28\n",
      "📁 Test Subject: sub-105 | 🟢 Train Acc: 0.65 | 🔵 Test Acc: 0.30\n",
      "📁 Test Subject: sub-107 | 🟢 Train Acc: 0.63 | 🔵 Test Acc: 0.33\n",
      "📁 Test Subject: sub-108 | 🟢 Train Acc: 0.63 | 🔵 Test Acc: 0.28\n",
      "📁 Test Subject: sub-109 | 🟢 Train Acc: 0.62 | 🔵 Test Acc: 0.30\n",
      "📁 Test Subject: sub-112 | 🟢 Train Acc: 0.65 | 🔵 Test Acc: 0.28\n",
      "📁 Test Subject: sub-113 | 🟢 Train Acc: 0.64 | 🔵 Test Acc: 0.23\n",
      "📁 Test Subject: sub-119 | 🟢 Train Acc: 0.64 | 🔵 Test Acc: 0.33\n",
      "📁 Test Subject: sub-120 | 🟢 Train Acc: 0.63 | 🔵 Test Acc: 0.25\n",
      "📁 Test Subject: sub-121 | 🟢 Train Acc: 0.63 | 🔵 Test Acc: 0.38\n",
      "📁 Test Subject: sub-123 | 🟢 Train Acc: 0.61 | 🔵 Test Acc: 0.25\n",
      "📁 Test Subject: sub-124 | 🟢 Train Acc: 0.62 | 🔵 Test Acc: 0.38\n",
      "📁 Test Subject: sub-125 | 🟢 Train Acc: 0.64 | 🔵 Test Acc: 0.42\n",
      "📁 Test Subject: sub-126 | 🟢 Train Acc: 0.65 | 🔵 Test Acc: 0.22\n",
      "📁 Test Subject: sub-127 | 🟢 Train Acc: 0.64 | 🔵 Test Acc: 0.30\n",
      "📁 Test Subject: sub-129 | 🟢 Train Acc: 0.62 | 🔵 Test Acc: 0.25\n",
      "📁 Test Subject: sub-130 | 🟢 Train Acc: 0.63 | 🔵 Test Acc: 0.28\n",
      "📁 Test Subject: sub-131 | 🟢 Train Acc: 0.64 | 🔵 Test Acc: 0.42\n",
      "📁 Test Subject: sub-133 | 🟢 Train Acc: 0.63 | 🔵 Test Acc: 0.20\n",
      "📁 Test Subject: sub-134 | 🟢 Train Acc: 0.63 | 🔵 Test Acc: 0.33\n",
      "📁 Test Subject: sub-139 | 🟢 Train Acc: 0.61 | 🔵 Test Acc: 0.35\n",
      "📁 Test Subject: sub-140 | 🟢 Train Acc: 0.64 | 🔵 Test Acc: 0.33\n",
      "📁 Test Subject: sub-141 | 🟢 Train Acc: 0.65 | 🔵 Test Acc: 0.50\n",
      "📁 Test Subject: sub-142 | 🟢 Train Acc: 0.63 | 🔵 Test Acc: 0.30\n",
      "📁 Test Subject: sub-143 | 🟢 Train Acc: 0.65 | 🔵 Test Acc: 0.33\n",
      "📁 Test Subject: sub-144 | 🟢 Train Acc: 0.64 | 🔵 Test Acc: 0.25\n",
      "📁 Test Subject: sub-145 | 🟢 Train Acc: 0.64 | 🔵 Test Acc: 0.30\n",
      "📁 Test Subject: sub-146 | 🟢 Train Acc: 0.63 | 🔵 Test Acc: 0.33\n",
      "📁 Test Subject: sub-147 | 🟢 Train Acc: 0.64 | 🔵 Test Acc: 0.35\n",
      "📁 Test Subject: sub-148 | 🟢 Train Acc: 0.61 | 🔵 Test Acc: 0.35\n",
      "📁 Test Subject: sub-149 | 🟢 Train Acc: 0.63 | 🔵 Test Acc: 0.30\n",
      "\n",
      "✅ Mean Train Accuracy: 0.63\n",
      "✅ Mean Test Accuracy (LOSO): 0.311\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from lightgbm import LGBMClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Run LOSO with Best Params\n",
    "best_params = study.best_trial.params\n",
    "best_params.update({\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': 4,\n",
    "   # 'metric': 'multi_logloss',\n",
    "    \n",
    "    'n_jobs': -1\n",
    "})\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "test_accuracies = []\n",
    "train_accuracies = []\n",
    "subject_ids = []\n",
    "\n",
    "for train_idx, test_idx in logo.split(X, y, groups=groups):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    # Save test subject\n",
    "    test_subject = groups.iloc[test_idx].iloc[0]\n",
    "    subject_ids.append(test_subject)\n",
    "\n",
    "    # Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Model training\n",
    "    model = LGBMClassifier(**best_params)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_train_pred = model.predict(X_train_scaled)\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    # Accuracies\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    train_accuracies.append(train_acc)\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "    print(f\"📁 Test Subject: {test_subject} | 🟢 Train Acc: {train_acc:.2f} | 🔵 Test Acc: {test_acc:.2f}\")\n",
    "\n",
    "# Summary\n",
    "mean_train = np.mean(train_accuracies)\n",
    "mean_test = np.mean(test_accuracies)\n",
    "\n",
    "print(f\"\\n✅ Mean Train Accuracy: {mean_train:.2f}\")\n",
    "print(f\"✅ Mean Test Accuracy (LOSO): {mean_test:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e832ce91-2746-4a82-9d46-ad4c8b0a17ad",
   "metadata": {},
   "source": [
    "## Exclude sub 125 for testing web application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b829808f-262d-4d3b-b3a6-4b69043ebadc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>label</th>\n",
       "      <th>S1_D1_w1</th>\n",
       "      <th>S1_D1_w2</th>\n",
       "      <th>S1_D1_w3</th>\n",
       "      <th>S1_D2_w1</th>\n",
       "      <th>S1_D2_w2</th>\n",
       "      <th>S1_D2_w3</th>\n",
       "      <th>S2_D1_w1</th>\n",
       "      <th>S2_D1_w2</th>\n",
       "      <th>...</th>\n",
       "      <th>S9_D6_w3</th>\n",
       "      <th>S9_D8_w1</th>\n",
       "      <th>S9_D8_w2</th>\n",
       "      <th>S9_D8_w3</th>\n",
       "      <th>S10_D7_w1</th>\n",
       "      <th>S10_D7_w2</th>\n",
       "      <th>S10_D7_w3</th>\n",
       "      <th>S10_D8_w1</th>\n",
       "      <th>S10_D8_w2</th>\n",
       "      <th>S10_D8_w3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sub-101</td>\n",
       "      <td>2</td>\n",
       "      <td>-7.120779e-08</td>\n",
       "      <td>-1.591482e-08</td>\n",
       "      <td>4.037125e-08</td>\n",
       "      <td>1.059506e-07</td>\n",
       "      <td>-1.591482e-08</td>\n",
       "      <td>5.576864e-09</td>\n",
       "      <td>-5.745610e-08</td>\n",
       "      <td>1.158969e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.338075e-08</td>\n",
       "      <td>4.680183e-08</td>\n",
       "      <td>-6.375286e-08</td>\n",
       "      <td>1.427296e-08</td>\n",
       "      <td>4.680183e-08</td>\n",
       "      <td>-7.726111e-09</td>\n",
       "      <td>1.427296e-08</td>\n",
       "      <td>-7.726111e-09</td>\n",
       "      <td>1.427296e-08</td>\n",
       "      <td>-7.726111e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sub-101</td>\n",
       "      <td>0</td>\n",
       "      <td>-7.105098e-08</td>\n",
       "      <td>-1.428847e-07</td>\n",
       "      <td>-1.661490e-07</td>\n",
       "      <td>-1.067971e-07</td>\n",
       "      <td>-1.428847e-07</td>\n",
       "      <td>-1.549473e-07</td>\n",
       "      <td>-1.100738e-07</td>\n",
       "      <td>-9.783376e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.825674e-08</td>\n",
       "      <td>-1.156452e-07</td>\n",
       "      <td>-1.293089e-07</td>\n",
       "      <td>-1.468958e-07</td>\n",
       "      <td>-1.156452e-07</td>\n",
       "      <td>-1.101812e-07</td>\n",
       "      <td>-1.468958e-07</td>\n",
       "      <td>-1.101812e-07</td>\n",
       "      <td>-1.468958e-07</td>\n",
       "      <td>-1.101812e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sub-101</td>\n",
       "      <td>3</td>\n",
       "      <td>6.854295e-08</td>\n",
       "      <td>1.563502e-07</td>\n",
       "      <td>1.207582e-07</td>\n",
       "      <td>1.066198e-07</td>\n",
       "      <td>1.563502e-07</td>\n",
       "      <td>1.385606e-07</td>\n",
       "      <td>1.212980e-07</td>\n",
       "      <td>4.591666e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>4.139978e-08</td>\n",
       "      <td>2.738268e-08</td>\n",
       "      <td>7.855582e-08</td>\n",
       "      <td>1.718898e-08</td>\n",
       "      <td>2.738268e-08</td>\n",
       "      <td>4.968935e-09</td>\n",
       "      <td>1.718898e-08</td>\n",
       "      <td>4.968935e-09</td>\n",
       "      <td>1.718898e-08</td>\n",
       "      <td>4.968935e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sub-101</td>\n",
       "      <td>1</td>\n",
       "      <td>1.206235e-07</td>\n",
       "      <td>1.431428e-07</td>\n",
       "      <td>2.246828e-08</td>\n",
       "      <td>2.433219e-08</td>\n",
       "      <td>1.431428e-07</td>\n",
       "      <td>1.236011e-07</td>\n",
       "      <td>1.035702e-07</td>\n",
       "      <td>1.868319e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.115965e-07</td>\n",
       "      <td>-2.759947e-08</td>\n",
       "      <td>-1.808735e-07</td>\n",
       "      <td>5.235214e-08</td>\n",
       "      <td>-2.759947e-08</td>\n",
       "      <td>1.151776e-08</td>\n",
       "      <td>5.235214e-08</td>\n",
       "      <td>1.151776e-08</td>\n",
       "      <td>5.235214e-08</td>\n",
       "      <td>1.151776e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sub-101</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.319688e-07</td>\n",
       "      <td>-1.376738e-07</td>\n",
       "      <td>-2.001670e-07</td>\n",
       "      <td>-1.707070e-07</td>\n",
       "      <td>-1.376738e-07</td>\n",
       "      <td>-6.784068e-08</td>\n",
       "      <td>-1.378276e-07</td>\n",
       "      <td>-6.926143e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.588279e-07</td>\n",
       "      <td>-1.109203e-07</td>\n",
       "      <td>-2.869832e-07</td>\n",
       "      <td>-1.373283e-07</td>\n",
       "      <td>-1.109203e-07</td>\n",
       "      <td>-1.356308e-07</td>\n",
       "      <td>-1.373283e-07</td>\n",
       "      <td>-1.356308e-07</td>\n",
       "      <td>-1.373283e-07</td>\n",
       "      <td>-1.356308e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject  label      S1_D1_w1      S1_D1_w2      S1_D1_w3      S1_D2_w1  \\\n",
       "0  sub-101      2 -7.120779e-08 -1.591482e-08  4.037125e-08  1.059506e-07   \n",
       "1  sub-101      0 -7.105098e-08 -1.428847e-07 -1.661490e-07 -1.067971e-07   \n",
       "2  sub-101      3  6.854295e-08  1.563502e-07  1.207582e-07  1.066198e-07   \n",
       "3  sub-101      1  1.206235e-07  1.431428e-07  2.246828e-08  2.433219e-08   \n",
       "4  sub-101      2 -1.319688e-07 -1.376738e-07 -2.001670e-07 -1.707070e-07   \n",
       "\n",
       "       S1_D2_w2      S1_D2_w3      S2_D1_w1      S2_D1_w2  ...      S9_D6_w3  \\\n",
       "0 -1.591482e-08  5.576864e-09 -5.745610e-08  1.158969e-09  ... -6.338075e-08   \n",
       "1 -1.428847e-07 -1.549473e-07 -1.100738e-07 -9.783376e-08  ... -9.825674e-08   \n",
       "2  1.563502e-07  1.385606e-07  1.212980e-07  4.591666e-08  ...  4.139978e-08   \n",
       "3  1.431428e-07  1.236011e-07  1.035702e-07  1.868319e-09  ... -1.115965e-07   \n",
       "4 -1.376738e-07 -6.784068e-08 -1.378276e-07 -6.926143e-08  ... -1.588279e-07   \n",
       "\n",
       "       S9_D8_w1      S9_D8_w2      S9_D8_w3     S10_D7_w1     S10_D7_w2  \\\n",
       "0  4.680183e-08 -6.375286e-08  1.427296e-08  4.680183e-08 -7.726111e-09   \n",
       "1 -1.156452e-07 -1.293089e-07 -1.468958e-07 -1.156452e-07 -1.101812e-07   \n",
       "2  2.738268e-08  7.855582e-08  1.718898e-08  2.738268e-08  4.968935e-09   \n",
       "3 -2.759947e-08 -1.808735e-07  5.235214e-08 -2.759947e-08  1.151776e-08   \n",
       "4 -1.109203e-07 -2.869832e-07 -1.373283e-07 -1.109203e-07 -1.356308e-07   \n",
       "\n",
       "      S10_D7_w3     S10_D8_w1     S10_D8_w2     S10_D8_w3  \n",
       "0  1.427296e-08 -7.726111e-09  1.427296e-08 -7.726111e-09  \n",
       "1 -1.468958e-07 -1.101812e-07 -1.468958e-07 -1.101812e-07  \n",
       "2  1.718898e-08  4.968935e-09  1.718898e-08  4.968935e-09  \n",
       "3  5.235214e-08  1.151776e-08  5.235214e-08  1.151776e-08  \n",
       "4 -1.373283e-07 -1.356308e-07 -1.373283e-07 -1.356308e-07  \n",
       "\n",
       "[5 rows x 74 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"all_subs_windowed.csv\")\n",
    "\n",
    "df['label'] = df['label'] - 1\n",
    "\n",
    "df = df[df['subject'] != 'sub-125']  # exclude subject 125\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "143efcd8-9a9b-4baf-bda4-de1a47bf8564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Test Subject: sub-101 | 🟢 Train Acc: 0.61 | 🔵 Test Acc: 0.30\n",
      "📁 Test Subject: sub-105 | 🟢 Train Acc: 0.63 | 🔵 Test Acc: 0.33\n",
      "📁 Test Subject: sub-107 | 🟢 Train Acc: 0.62 | 🔵 Test Acc: 0.28\n",
      "📁 Test Subject: sub-108 | 🟢 Train Acc: 0.63 | 🔵 Test Acc: 0.35\n",
      "📁 Test Subject: sub-109 | 🟢 Train Acc: 0.63 | 🔵 Test Acc: 0.25\n",
      "📁 Test Subject: sub-112 | 🟢 Train Acc: 0.63 | 🔵 Test Acc: 0.33\n",
      "📁 Test Subject: sub-113 | 🟢 Train Acc: 0.63 | 🔵 Test Acc: 0.23\n",
      "📁 Test Subject: sub-119 | 🟢 Train Acc: 0.60 | 🔵 Test Acc: 0.28\n",
      "📁 Test Subject: sub-120 | 🟢 Train Acc: 0.62 | 🔵 Test Acc: 0.25\n",
      "📁 Test Subject: sub-121 | 🟢 Train Acc: 0.62 | 🔵 Test Acc: 0.25\n",
      "📁 Test Subject: sub-123 | 🟢 Train Acc: 0.60 | 🔵 Test Acc: 0.28\n",
      "📁 Test Subject: sub-124 | 🟢 Train Acc: 0.61 | 🔵 Test Acc: 0.38\n",
      "📁 Test Subject: sub-126 | 🟢 Train Acc: 0.63 | 🔵 Test Acc: 0.22\n",
      "📁 Test Subject: sub-127 | 🟢 Train Acc: 0.63 | 🔵 Test Acc: 0.28\n",
      "📁 Test Subject: sub-129 | 🟢 Train Acc: 0.60 | 🔵 Test Acc: 0.25\n",
      "📁 Test Subject: sub-130 | 🟢 Train Acc: 0.61 | 🔵 Test Acc: 0.35\n",
      "📁 Test Subject: sub-131 | 🟢 Train Acc: 0.61 | 🔵 Test Acc: 0.23\n",
      "📁 Test Subject: sub-133 | 🟢 Train Acc: 0.63 | 🔵 Test Acc: 0.23\n",
      "📁 Test Subject: sub-134 | 🟢 Train Acc: 0.63 | 🔵 Test Acc: 0.28\n",
      "📁 Test Subject: sub-139 | 🟢 Train Acc: 0.61 | 🔵 Test Acc: 0.35\n",
      "📁 Test Subject: sub-140 | 🟢 Train Acc: 0.65 | 🔵 Test Acc: 0.20\n",
      "📁 Test Subject: sub-141 | 🟢 Train Acc: 0.62 | 🔵 Test Acc: 0.45\n",
      "📁 Test Subject: sub-142 | 🟢 Train Acc: 0.59 | 🔵 Test Acc: 0.25\n",
      "📁 Test Subject: sub-143 | 🟢 Train Acc: 0.63 | 🔵 Test Acc: 0.33\n",
      "📁 Test Subject: sub-144 | 🟢 Train Acc: 0.61 | 🔵 Test Acc: 0.30\n",
      "📁 Test Subject: sub-145 | 🟢 Train Acc: 0.61 | 🔵 Test Acc: 0.25\n",
      "📁 Test Subject: sub-146 | 🟢 Train Acc: 0.61 | 🔵 Test Acc: 0.25\n",
      "📁 Test Subject: sub-147 | 🟢 Train Acc: 0.64 | 🔵 Test Acc: 0.33\n",
      "📁 Test Subject: sub-148 | 🟢 Train Acc: 0.63 | 🔵 Test Acc: 0.45\n",
      "📁 Test Subject: sub-149 | 🟢 Train Acc: 0.62 | 🔵 Test Acc: 0.33\n",
      "\n",
      "✅ Mean Train Accuracy: 0.62\n",
      "✅ Mean Test Accuracy (LOSO): 0.292\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from lightgbm import LGBMClassifier\n",
    "import numpy as np\n",
    "\n",
    "X = df.drop(columns=['subject', 'label'])\n",
    "y = df['label']\n",
    "groups = df['subject']\n",
    "\n",
    "# Run LOSO with Best Params, the same parameters from training with all data\n",
    "best_params = {'n_estimators': 158, 'max_depth': 14, 'num_leaves': 120, 'learning_rate': 0.0023129559233807115, \n",
    "                'feature_fraction': 0.8181304692531582, 'bagging_fraction': 0.9302593262691019, \n",
    "                'lambda_l1': 4.583363901282219, 'lambda_l2': 1.9891249768767543, 'min_child_samples': 43, \n",
    "                'max_bin': 248, 'objective': 'multiclass', 'num_class': 4, \n",
    "               # 'metric': 'multi_logloss', \n",
    "               'n_estimators': 100, 'n_jobs': -1}\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "test_accuracies = []\n",
    "train_accuracies = []\n",
    "subject_ids = []\n",
    "\n",
    "for train_idx, test_idx in logo.split(X, y, groups=groups):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    # Save test subject\n",
    "    test_subject = groups.iloc[test_idx].iloc[0]\n",
    "    subject_ids.append(test_subject)\n",
    "\n",
    "    # Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Model training\n",
    "    model = LGBMClassifier(**best_params)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_train_pred = model.predict(X_train_scaled)\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    # Accuracies\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    train_accuracies.append(train_acc)\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "    print(f\"📁 Test Subject: {test_subject} | 🟢 Train Acc: {train_acc:.2f} | 🔵 Test Acc: {test_acc:.2f}\")\n",
    "\n",
    "# Summary\n",
    "mean_train = np.mean(train_accuracies)\n",
    "mean_test = np.mean(test_accuracies)\n",
    "\n",
    "print(f\"\\n✅ Mean Train Accuracy: {mean_train:.2f}\")\n",
    "print(f\"✅ Mean Test Accuracy (LOSO): {mean_test:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3bbb1e1f-b3cb-4640-bded-88fe860db164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model for app test\n",
    "\n",
    "model.booster_.save_model(\"4Class_best_lgbm_loso_029.txt\")\n",
    "\n",
    "np.save(\"scaler2_mean.npy\", scaler.mean_)\n",
    "np.save(\"scaler2_scale.npy\", scaler.scale_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
